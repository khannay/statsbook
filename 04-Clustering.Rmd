# Introduction to Clustering



## What is Clustering?
Clustering properly belongs in a class on Machine Learning. However, it is so useful we will cover just the basics in this class. The idea behind clustering is to look for groups or **clusters** of related data points in our data set. For example, we might have a data set which gives the purchase history of customers. From this data we might want to extract classes of customers.

Here is what we need to get started (plus my package as usual):
```{r message=FALSE}
library(maps)
library(cluster)
```
these packages should have been installed at the same time you installed my package, if not then you will have to install them using the install.packages() function. 

 
## Introduction to Kmeans clustering
We will be using the kmeans clustering algorithm. The kmeans algorithm is pretty intuitive in nature. For example, lets suppose we have collected a data set with two variables and wish to create two clusters. We could plot the data points using those two variables:
```{r, echo=FALSE}
mysd=0.25
x1<-rnorm(100, 1, sd=mysd)
y1<-rnorm(100, 1, sd=mysd)
x2<-rnorm(100,0, sd=mysd)
y2<-rnorm(100,0,sd=mysd)
x<-c(x1,x2)
y<-c(y1,y2)
practice_cluster=data.frame(x=x, y=y)
plot(practice_cluster$x, practice_cluster$y, main='K means Clustering Ideas')
```

We can intuitively see two clusters in this data set. One is centered around (0,0) and the other is centered around (1,1). We can then draw a circles with a radius large enough to contain those points which are close to those centers. This is the main idea behind the k means clustering algorithm. However instead of us trying to eyeball where the center of the circles are we let the algorithm do the work for us. The k means algorithm has to be told how many clusters we are looking for, it then proceeds to minimize the sum of distances of points in a cluster to those centers. 

```{r}
cluster_obj<-kmeans(practice_cluster, centers=2, nstart=25)
clusplot(practice_cluster, cluster_obj$cluster)
```

This makes a two-dimensional plot of our data and which cluster each data point is assigned to. We can see that is this case the kmeans algorithm does a pretty good job of finding the two clusters in the data. 


#### Crime Clusters by State
We are now going to look for clusters in a real data set. Lets load in a crime data set, which gives murder, assault and rape statistics for all 50 states in 1973. We are going to see if we can cluster these into groups of high and low crime states. 
```{r}
data("crime_cluster")
```
Looking at the help page for this data we can see that we are given the number of arrests in each category per 100,000 residents. 
```{r}
head(crime_cluster)
```
You can see that there are many more assault arrests per 100,000 residents than the other two categories. This can cause some issues with our clustering. Therefore, it is generally a good idea to **standardize** your variables. Recall, this means we just transform them so that each column has a mean of zero and a standard deviation of one. The `scale` command in `R` does this for us easily. For example,
```{r}
ccs=data.frame(sapply(crime_cluster, scale)) ##this scales each column of the data and makes a new data frame ccs
rownames(ccs)=rownames(crime_cluster) ##this makes sure the state names are in our standardized data set
```
Now when we look at the data:
```{r}
head(ccs)
```
we can see that Alabama is about one standard deviation above the national average in the murder and assault rate and about average in the rape category. 

```{block2, type='warning'}
When performing a cluster abalysis on a data set you should consider standardizing your variables. This will allow for easier interpretations of the cluster centers and will prevent one variable (column) from dominating the cluster algorithm. 
```


We can make a map of our crime clusters of states in `R`, using a function I wrote called StatePlot. 

```{r}
cl2<-StatePlot(2, ccs) ##2 is the number of clusters to look for, ccs is the data
```

Notice this prints out the cluster centers. It looks like it has found two main groups: high crime and low crime. The high crime cluster are all about one standard deviation above the mean in each of the three categories. The low crime cluster is centered around being 0.6 standard deviations below the national average in all categories. 

We can explore the differences between the clusters further by making some box plots, splitting the states into their clusters. For example, the below box plot compares the normalized murder rates between the two clusters. 
```{r}
boxplot(ccs$Murder~cl2$cluster, main='Murder Rate for the Two Clusters', ylab='Murder Rate', xlab='Cluster', col='coral')
```


We can also make a two-dimensional plot of our clusters using the `clusplot` command (in the cluster package). Since we have three variables this two-dimensional plot is a projection (shadow). `R` automatically chooses the best way to project the data onto two dimensions. 

```{r}
clusplot(ccs,cl2$cluster, labels=3, color=TRUE)
```

Looking at this plot lets us see which states are barely in the high or low crime clusters (and which are NOT!). For example, it looks like Missouri just barely makes the list of high crime states according to our analysis. 



## How many clusters should we choose?

A difficult question to answer when we are conducting a cluster analysis on data is: How many clusters should I pick to get the best representation of my data? Sometimes, we know that we are looking for some number of groups. For example, cancer genes and not cancer genes, or terrorist versus non-terrorist, etc. However, in many other cases it is not obvious how many clusters should be in our data. For example, how many customer types are shopping on your website, how many types of learners are in the classroom, etc. Part of the beauty of cluster analysis is that we let the data guide us to how many clusters to pick. 

To begin lets look at the crime data, and see what happens if we divide states into three groups. Here is a map if we cluster the states into three groups:

```{r}
cl3<-StatePlot(3, ccs)
```

We can see that the new cluster mostly split the low crime states into very low and sort of low. Here is a look at the clusters if we split the data into four clusters. 
```{r}
cl4<-StatePlot(4, ccs)
```

In general, if we split the data into more clusters we can expect the data points to lie closer to the centers of the clusters. We can measure this by looking at the sum of all distances between the data points and the center of their clusters. The `kmeans` function reports this value to use:
```{r}
cl2$tot.withinss
cl3$tot.withinss
cl4$tot.withinss
```

This will decrease as we increase the number of clusters. If we allowed for 50 clusters we would just get one cluster for each state in our data set (giving a withinss value of zero)-- although this wouldn't really tell us any useful information. Lets make a plot of the `tot.withinss` or distortion measurements against the total number of clusters. 

```{r}
ElbowClusterPlot(ccs) ##This is a special function written by me in the package HannayAppliedStats
```

One common criteria for choosing the number of clusters to use is to look for the "elbow" for this plot. The elbow gives the smallest number of clusters which yields a big decrease in the total distance from the centers of the clusters. For the crime clusters the elbow occurs for $2$ clusters, as adding in a third cluster doesn't really reduce the total error (distortion) by much. 

```{block2, type='note'}
Use an **elbow** plot to look for the number of clusters you can form from a data set. This should be balanced against the number of clusters you expect to find in your data set. 
```


```{exercise}
Conduct a cluster analysis for the bad drivers data set. Load this data by typing `data(bad_drivers_cluster)`.
  + Make a State plot of the bad drivers data set for two clusters. Does Texas belong to the better or worse driver cluster?
  + Cluster the States into two clusters and make a boxplot comparing the perc_speeding values between the clusters
  + Make an Elbow plot to determine the optimal number of clusters in the data. Does this plot have an obvious elbow?
```


## Clustering NBA Players

As another interesting application of clustering lets consider clustering the top 100 NBA players by per game statistics. The below code forms two clusters among the top 100 players, using a built in data set:

```{r}
data("nba_pg_2016") ##load the nba data
nba_clusters=kmeans(nba_pg_2016, centers=2, nstart=25)
nba_clusters$centers
row.names(subset(nba_pg_2016, nba_clusters$cluster==1))
row.names(subset(nba_pg_2016, nba_clusters$cluster==2))
```

Knowing something about the NBA it looks like the clustering algorithm has found the a cluster of the "star" players. We could also view this as high usage players versus low usage players. The star cluster gets more shot attempts, free throws, etc then the other cluster. Here is a two dimensional plot of the two cluster solution. 

```{r}
clusplot(nba_pg_2016, nba_clusters$cluster)
```

Lets see what happens if we break into three clusters:

```{r}
nba_clusters=kmeans(nba_pg_2016, centers=3, nstart=25)
nba_clusters$centers
row.names(subset(nba_pg_2016, nba_clusters$cluster==1))
row.names(subset(nba_pg_2016, nba_clusters$cluster==2))
row.names(subset(nba_pg_2016, nba_clusters$cluster==3))
```

Looks like the "high usage" or stars split into two clusters (mid level stars and superstars) when we allow for three clusters. Conducting an elbow plot analysis shows that two or three clusters is probably the best choice in this case. 

```{r}
ElbowClusterPlot(nba_pg_2016, scale = FALSE)
```

## Requirements for Performing Cluster Analysis
In order to perform kmeans clustering analysis on a data set we need to have a a key property:

* All the data used in the clustering must be either numerical in nature or at least an ordinal categorical variable (stored as a number, with a defined order). You cannot use clustering analysis on data which includes nominal categorical variables as the **distance** between categories like (male/female) isn't defined. I have written a function called `grabNumeric` in my package which can be used to remove any non numerical columns from a data frame. By default this will also drop the rows with missing values in the clustering variables.

See the example below for how to use the grabNumeric function. 

```{r}
data("Young_People_Survey")
dim(Young_People_Survey)
yp=grabNumeric(Young_People_Survey)
dim(yp)
```

* The clusters found by the algorithm can be sensitive to the normalization of the data. You should choose whether you want to standardize your variables (using `scale`) carefully. Having variables which were measured on widely different scales can lead to erroneous clusters being found.


## Homework 

### Concept Questions

Are the following statements True or False? Why?

1. When performing a kmeans cluster analysis, the algorithm will automatically choose the optimal number of clusters for you.

2. Cluster analysis can be performed using nominal categorical variables.

3. When performing cluster analysis you should **always** standardize the variables. 

4. Kmeans clustering seeks to minimise the distance from each point to the center of a fixed number of clusters.


### Practice Problems

1. Give an example of a data set where clustering analysis might be interesting. This can be an imaginary data set, just explain the context. 


### Advanced Problems

1. Load the bad_drivers data set and perform a cluster analysis. 

    + Should we standardize the variables in this data set?
    + How many clusters should we choose for this data set? 
    + Which states have the worst drivers? Give just a couple of examples of states in the cluster with the worst drivers. 

2. Load the iris data set using `data(iris)`. Look at the help page to see what this data set contains `?iris`.

    + How many iris species are in the data set? 
    + Can the Species column be used in a clustering analysis? Why or Why not?
    + If the Species column can not be used in your estimation remove this column by running the command: ```iris$Species<-NULL```. How many clusters should we choose for this data set? 
    + Run a clustering analysis and give the centroids of your clusters
    + Make a `clusplot` of your clusters. How well are they separated? You will need the cluster package for this installed and loaded.
    + Make a box plot of the Sepal.Width for the different clusters found in your analysis. 
















