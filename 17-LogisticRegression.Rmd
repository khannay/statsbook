# (PART) Advanced Regression Topics {-} 

# Logistic Regression

```{r, echo=FALSE}
library(tidyverse)
library(HannayIntroStats)
library(caret)
data("shot_logs_2014")
invlogit<-function(x) {1/(1+exp(-1*x));}
logit<- function(x) {log(x/(1-x));}
```

## What is logistic regression used for?
Logistic regression is useful when we have a *response* variable which is categorical with only two categories. This might seem like it wouldn't be especially useful, however with a little thought we can see that this is actually a very useful thing to know how to do. Here are some examples where we might use **logistic regression**.

  + Predict whether a customer will visit your website again using browsing data
  + Predict whether a voter will vote for the democratic candidate in an upcoming election using demographic and polling data
  + Predict whether a patient given a surgery will survive for 5+ years after the surgery using health data
  + Given the history of a stock, market trends predict if the closing price tomorrow will be higher or lower than today?
  
With many other possible examples. We can often phrase important questions as yes/no or (0-1) answers where we want to use some data to better predict the outcome. This is a simple case of what is called a *classification* problem in the machine learning/data science community. Given some information we want to use a computer to decide make a prediction which can be sorted into some finite number of outcomes.  

## GLM: Generalized Linear Models
Our linear regression techniques thus far have focused on cases where the response ($Y$) variable is continuous in nature. Recall, they take the form:
$$
Y_i=\alpha+ \sum_{j=1}^N \beta_j X_{ij}
$$
Where $alpha$ is the intercept and $\{\beta_1, \beta_2, ... \beta_N\}$ are the slope parameters for the explanatory variables ($\{X_1, X_2, ...X_N\}$). However, our outputs $Y_i$ should give the probability that $Y_i$ takes the value 1 given the $X_j$ values. The right hand side of our model above will produce values in $\mathbb{R}=(-\infty, \infty)$ while the left hand side *should* live in $[0,1]$. 

Therefore to use a model like this we need to *transform* our outputs from [0,1] to the whole real line $\mathbb{R}$. 

The **logit** function is useful for this purpose as it maps $logit: [0,1] \rightarrow \mathbb{R}$. The logit function takes the form:
$$logit(x)=\ln \left(\frac{x}{1-x}\right)$$
```{r, plotLogit, echo=FALSE}
logit <- function(x) { log(x/(1-x))};
x<-seq(0,1,0.01);
y<-logit(x)
plot(x,y,type='l', main='Logit Function', xlab='x', ylab='logit(x)')
```
Thus, we can use our multiple regression techniques if treat our outputs as $Y_i=logit(p_i)$. This is the basic idea of logistic regression:
$$
\begin{aligned}
& Y_i=logit(p_i)=\alpha+ \sum_{j=1}^N \beta_j X_{ij}
\end{aligned}
$$
Usually, we want to know $p_i$ and not $logit(p_i)$ and we can find this using the *inverse logit* $logit^{-1}$. 
$$
\begin{aligned}
& p_i=logit^{-1} \left( \alpha+ \sum_{j=1}^N \beta_j X_{ij}   \right) \\
& logit^{-1}(\gamma)=\frac{1}{1+e^{-\gamma}}
\end{aligned}
$$
A logistic regression is one example in a family of techniques called **generalized linear models (GLM)**. GLMs involve a linear predictor function $\alpha+ \sum_{j=1}^N \beta_j X_{ij}$ and a *link function* $g()$ which maps the linear predictor to the response variable. 

$$y_i=g \left( \alpha+ \sum_{j=1}^N \beta_j X_{ij} \right)$$ 

## A Starting Example
Let's consider the shot logs data set again. We will use the shot distance column SHOT_DIST and the FGM columns for a logistic regression. The FGM column is 1 if the shot was made and 0 otherwise (perfect candidate for the response variable in a logistic regression). We expect that the further the shot is from the basket (SHOT_DIST) the less likely it will be that the shot is made (FGM=1). 

To build this model in R we will use the `glm()` command and specify the link function we are using a the logit function. 

```{r}
logistic.nba<-glm(FGM~SHOT_DIST, data=shot_logs_2014, family=binomial(link="logit"))
logistic.nba$coefficients
```

$$logit(p)=0.392-0.04 \times SD \implies p=logit^{-1}(0.392-0.04 \times SD)$$
So we can find the probability of a shot going in 12 feet from the basket as:
```{r}
invlogit(0.392-0.04*12)
```

Here is a plot of the probability of a shot going in as a function of the distance from the basket using our best fit coefficients. 

```{r}
x<-seq(0, 400, 40.0)
p<-invlogit(0.392-0.04*x)
slplot=dplyr::sample_n(shot_logs_2014, 1000)
plot(slplot$SHOT_DIST, slplot$FGM+rnorm(dim(slplot)[1], sd=0.01), cex=0.15, xlab='Shot Distance (Feet)', ylab='Probability of Shot Being Made')
lines(x,p, type = 'l', col='red')
```

### Confidence Intervals for the Parameters

A major point of this book is that you should never be satisfied with a single number summary in statistics. Rather than just considering a single best fit for our coefficients we should really form some confidence intervals for their values. 

As we saw for simple regression we can look at the confidence intervals for our intercepts and slopes using the `confint` command. 

```{r, cache=TRUE, warning=FALSE}
library(MASS)
ci1<-confint(logistic.nba)
print(ci1)
```
Note, these values are still in the `logit` transformed scale. 

## Equivalence of Logistic Regression and Proportion Tests

Suppose we want to use the categorical variable of the individual player in our analysis. In the interest of keeping our tables and graphs visible we will limit our players to just those who took more than 820 shots in the data set. 

```{r}
library(dplyr)
lotsShots<-shot_logs_2014 %>% group_by(player_name) %>% summarise(num.attempts=n()) %>% filter(num.attempts>=820) %>% arrange(player_name)
knitr::kable(lotsShots, col.names = c('Name', 'Number of Shots'))
```


Now we can get a reduced data set with just these players.

```{r}
sl2<-shot_logs_2014 %>% filter(player_name %in% lotsShots$player_name) %>% arrange(player_name)
```


Let's form a logistic regression using just a categorical variable as the explanatory variable. 
$$
logit(p)=\beta Player
$$


```{r}
logistic.nba.player<-glm(FGM~player_name+0, data=sl2, family=binomial(link="logit"))
summary(logistic.nba.player)
```

If we take the inverse logit of the coefficients we get the field goal percentage of the players in our data set. 

```{r}
invlogit(logistic.nba.player$coefficients)
```

Now suppose we want to see if the players in our data set truly differ in their field goal percentages or whether the differences we observe could just be caused by random effects. To do this we want to compare a model without the players information included with one that includes this information. Let's create a null model to compare against our player model. 

```{r}
null.player.model<-glm(FGM~1, data=sl2, family=binomial(link="logit"))
```

This null model contains no explanatory variables and takes the form: $$logit(p_i)=\alpha$$

Thus, the shooting percentage is not allowed to vary between the players. We find based on this data an overall field goal percentage of:

```{r}
invlogit(null.player.model$coefficients)
```

Now we may compare logistic regression models using the `anova` command in R. 

```{r}
anova(null.player.model, logistic.nba.player, test='LRT')
```

The second line contains a p value of 2.33e-5 telling us to reject the null hypothesis that the two models are equivalent. So we found that knowledge of the player does matter in calculating the probability of a shot being made. 


Notice we could have performed this analysis as a proportion test using the null that all players shooting percentages are the same $p_1=p_2=...p_{15}$


```{r}
prop.test(table(sl2$player_name, sl2$FGM))
```

Notice the p-value obtained matches the logistic regression ANOVA almost exactly. Thus, a proportion test can be viewed as a special case of a logistic regression. 

## Example: Building a More Accurate Model

Now we can form a model for the shooting percentages using the individual players data:

$$ logit(p_i)=\alpha+\beta_1 SF+\beta_{2} DD+\beta_3 (player_dummy) $$


```{r}
logistic.nba2<-glm(FGM~SHOT_DIST+Team.Defending, data=shot_logs_2014, family=binomial(link="logit"))
summary(logistic.nba2)
```


## Example: Measuring Team Defense Using Logistic Regression

$$
logit(p_i)=\alpha+\beta_1 SD+\beta_2 (Team)+\beta_3 (Team) (SD)
$$
Since the team defending is a categorical variable `R` will store it as a dummy variable when forming the regression. Thus the first level of this variable will not appear in our regression (or more precisely it will be included in the intercept $\alpha$ and slope $\beta_1$). Before we run the model we can see which team will be missing.

```{r}
levels(shot_logs_2014$Team.Defending)[1]
```

```{r}
logistic.nba.team<-glm(FGM~SHOT_DIST+Team.Defending+Team.Defending:SHOT_DIST, data=shot_logs_2014, family=binomial(link="logit"))
summary(logistic.nba.team)
```


The below plot shows the expected shooting percentages at each distance for the teams in the data set. 

```{r, echo=FALSE}
getTeamPredictions <- function(teamName) {
  res<-predict(logistic.nba.team, newdata=data.frame('SHOT_DIST'=seq(0,30,0.5), Team.Defending=rep(teamName,61)), se.fit=TRUE)
  mydf<-data.frame(xvalues=seq(0,30,0.5), Team=rep(teamName,61), Results=invlogit(res$fit), se=res$se.fit)
  return(mydf)
}
teamdf<-getTeamPredictions(levels(shot_logs_2014$Team.Defending)[1])
for (i in 2:30) {
  team=levels(shot_logs_2014$Team.Defending)[i]
  df=getTeamPredictions(team)
  teamdf=rbind(teamdf,df)
}

ggplot(data=teamdf, aes(x=xvalues, y=Results, color=Team))+geom_line()+xlab('Shot Distance (FT)')+ylab('Probability of Made Shot')+labs(title='Team Defense')

```

```{r}
#Make a ribbon plot including the errors in the predictions

plotTeamsSP<- function(teamList) {
  teamdf %>% filter(Team %in% teamList) %>% mutate(upper.prob=invlogit(logit(Results)+2*se),lower.prob=invlogit(logit(Results)-2*se))%>% ggplot(aes(x=xvalues, y=Results, color=Team))+geom_line()+geom_ribbon(aes(ymin=lower.prob, ymax=upper.prob, color=Team, fill=Team), alpha=0.2)+xlab('Shot Distance (FT)')+ylab('Probability of Made Shot')+labs(title='Team Defense')
}

plotTeamsSP(c("GSW", "SAS"))
```


#Better Approach

```{r}
inTraining<-createDataPartition(shot_logs_2014$FGM, p=0.80, list=FALSE)
sl_train<-shot_logs_2014[inTraining,]
sl_test <-shot_logs_2014[-inTraining,]
```


```{r}
#Build a logistic regression model using the training data
lr_sd<-glm(FGM~SHOT_DIST+as.factor(player_name), data=sl_train, family = binomial(link="logit"))

predicted.train<-ifelse(invlogit(predict.glm(lr_sd, sl_train))<0.5, "Made", "Missed")
actual.train<-ifelse(sl_train$FGM==1, "Made", "Missed")


confusionMatrix(as.factor(predicted.train), as.factor(actual.train))
```



















